
# coding: utf-8

# In[26]:


import tensorflow as tf
import numpy as np
from tensorflow.python.framework import ops
import cv2
import os
from sklearn.utils import shuffle
import pandas
import random
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from utils import *


# In[27]:


global params
params = {"batch_size":128, "lr_1":1e-3, "lr_2":1e-5, "input_size":(70,70), "classes":42, "epochs":50000, 
         "path" : '/home/hackerend/ML/DABanglaHindi/IndicCharacter_SSH/'
         }


# In[28]:


conv = tf.layers.conv2d
maxpool = tf.layers.max_pooling2d
Relu = tf.nn.relu
lays = tf.contrib.layers


# In[29]:


def get_file_paths(path):
    path = shuffle(sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path) for file in files]))
    path_train, path_test = path[:int(0.8*len(path))], path[int(0.8*len(path)):]
    return path_train, path_test


# In[30]:


def get_domain_label(im_paths):
    a = np.array([i.split('/')[-2] for i in im_paths])
    lang = np.unique(a)

    for i in range(len(lang)):
   

        a[a==lang[i]]=int(i)

    a = np.array(a.astype(int))
    return (np.arange(2) == a[:,None]).astype(np.float32)


# In[31]:


def load_data(path):
    img = cv2.imread(path, 0)
    if img is None: 

        return (False, None)
    else:
        img = cv2.resize(img, 
                         params['input_size'], 
                         cv2.INTER_AREA)
        return np.expand_dims(img, -1)


# In[32]:


from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
from tensorflow.python.framework import ops


class FlipGradientBuilder(object):
    def __init__(self):
        self.num_calls = 0

    def __call__(self, x, l=1.0):
        grad_name = "FlipGradient%d" % self.num_calls
        @ops.RegisterGradient(grad_name)
        def _flip_gradients(op, grad):
            return [tf.negative(grad) * l]
        
        g = tf.get_default_graph()
        with g.gradient_override_map({"Identity": grad_name}):
            y = tf.identity(x)
            
        self.num_calls += 1
        return y
    
flip_gradient = FlipGradientBuilder()


# In[33]:


def get_batch_label(im_paths, root):
    l = len(root)
    a = [t[l:] for t in im_paths]
    a = [int(t[:t.find('/')]) for t in a]

    arr = np.zeros((len(im_paths),  params['classes']))

    for i in range(arr.shape[0]):
        
        arr[i][a[i]-1] = 1
    return arr


# In[62]:


def ConvPool_4(input):
    #net = conv(input, filters=64, kernel_size=4, strides=1, padding='SAME', activation=Relu, name="C1")
    #net = maxpool(net, pool_size=4, strides=2, padding='SAME', name="P1")
    #net = conv(net, filters=64, kernel_size=4, strides=1, padding='SAME', activation=Relu, name="C2")
    #net = maxpool(net, pool_size=4, strides=2, padding='SAME', name="P2")
    #net = conv(net, filters=64, kernel_size=4, strides=1, padding='SAME', activation=Relu, name="C3")
    #net = maxpool(net, pool_size=4, strides=2, padding='SAME', name="P3")
    #net = conv(net, filters=64, kernel_size=4, strides=1, padding='SAME', activation=Relu, name="C4")
    #net = maxpool(net, pool_size=4, strides=2, padding='SAME', name="P4")
    W_conv0 = weight_variable([5, 5, 1, 32])
    b_conv0 = bias_variable([32])
    h_conv0 = tf.nn.relu(conv2d(input, W_conv0) + b_conv0)
    h_pool0 = max_pool_2x2(h_conv0)
            
    W_conv1 = weight_variable([5, 5, 32, 48])
    b_conv1 = bias_variable([48])
    h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)
            
            # The domain-invariant feature
    net = h_pool1
    return lays.flatten(net)


# In[63]:


def Classifier(input):
    
    F_space = tf.layers.dense(inputs=input, activation=Relu, units=1500, name="fc1")
    net = tf.layers.dense(inputs=F_space, activation=None, units= params['classes'], name="fc2")
    return F_space, net


# In[64]:


def Domain_discriminator(input):
    net = flip_gradient(input,0.1)
    net = tf.layers.dense(inputs=net, activation=Relu, units=1500, name="fc3")
    logit = tf.layers.dense(inputs=net, activation=None, units= 2, name="fc4")
    return tf.nn.softmax(logit), logit


# In[65]:


def classification_loss_function(logit, Label):
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit, labels = Label))
    return loss


# In[66]:


def domain_classification_loss_function(logit, Label):
    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit, labels = Label))
    return loss


# In[67]:


def Accuracy_Evaluate(prediction, Label):
    # Evaluate model
    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Label, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
    return correct_pred, accuracy


# In[68]:


def Model(lr_rate = 1e-5):
    
    w = params["input_size"][0]
    Input = tf.placeholder(tf.float32, shape=[None, w, w, 1])
    Label = tf.placeholder(tf.float32, shape=[None, params['classes']])
    Domain_label = tf.placeholder(tf.float32, shape=[None, 2])
    
    fet = ConvPool_4(Input)
    F_space, logit1 = Classifier(fet)
    pred, logit2 = Domain_discriminator(fet)
    
    
    loss = classification_loss_function(logit1, Label) + domain_classification_loss_function(logit2, Domain_label)
    prediction = tf.nn.softmax(logit1)
    correct_pred, accuracy = Accuracy_Evaluate(prediction, Label)
    optimiz = tf.train.RMSPropOptimizer(learning_rate = 0.0001).minimize(loss)
    
    tf.summary.scalar('loss', loss)
    tf.summary.scalar('accuracy', accuracy)
    summary_op = tf.summary.merge_all() 
    
    batchsize = params['batch_size']
    train_paths, test_paths = get_file_paths(params["path"])
    n_batches = len(train_paths)//batchsize

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)
    
    #seq = Augment_Pipeline()
    
    summary_writer = tf.summary.FileWriter('./tf_logs/sldcnn/train-basic', sess.graph)
    test_summary_writer = tf.summary.FileWriter('./tf_logs/sldcnn/test-basic', sess.graph)
    
    for epoch in range(params['epochs']):
        
        for idx in range(n_batches):
            
            batch_paths = train_paths[idx * batchsize: (idx+1) * batchsize]
            batchx = np.array([load_data(path) for path in batch_paths])
            batchy = get_batch_label(batch_paths, params["path"])
            batchy_domain = get_domain_label(batch_paths)
            
            feed_dict = {Input : batchx , Label : batchy, Domain_label : batchy_domain}
            
            _, train_loss, train_accuracy, summary_str = sess.run([optimiz, loss, accuracy, summary_op] , feed_dict )
            summary_writer.add_summary(summary_str, idx+1)
            
        if epoch%1 == 0:

            print ('epoch : '+str(epoch)+' train_loss : '+str(train_loss) +
                    ' train_accuracy : '+str(train_accuracy) 

                  )            
        if epoch%20 == 0 and epoch !=0:
            
            imgs = []
            
            labels = []

            Feature_spaces = []

            test_losses = []

            test_accues = []

            n_batches_test = len(test_paths)//batchsize

            for i in range(n_batches_test):

                batch_paths_test = test_paths[i * batchsize: (i+1) * batchsize]

                batchx_test = np.array([load_data(path) for path in batch_paths_test])

                batchy_test = get_batch_label(batch_paths_test, params["path"])
                
                batchy_domain_test = get_domain_label(batch_paths_test)

                feed_dict = {Input : batchx_test , Label : batchy_test, Domain_label : batchy_domain_test}

                test_loss, test_accuracy, F_space_test = sess.run([ loss, accuracy, F_space] , feed_dict)

                imgs.append(batchx_test)
                labels.extend(batchy_test)
                test_losses.append(test_loss)
                test_accues.append(test_accuracy)
                Feature_spaces.extend(F_space_test)
                
            test_loss_mean = np.mean(test_losses)
            
            test_accu_mean = np.mean(test_accues)
            
            Feature_spaces = np.array(Feature_spaces)
            
            
            
            
            plot_embedding(imgs, labels, Feature_spaces)
            
            print ('######## Test Results #########')
            print ('epoch : '+str(epoch)+' test_loss : '+str(test_loss_mean) +
                    ' test_accuracy : '+str(test_accu_mean)) 
            print ('######## Test Results #########')
            
            
            
            
                    
                    
                    
                    
                    
                    
                    
                    
                    


# In[69]:


def plot_embedding(imgs, labels, Feature_spaces):
   
    proj_Feature_spaces = TSNE(n_components=2).fit_transform(Feature_spaces)
    
    plt.scatter(proj_Feature_spaces.T[0],proj_Feature_spaces.T[1],c = np.argmax(labels,-1),cmap=plt.cm.jet)
    plt.show()


# In[ ]:


tf.reset_default_graph()

Model(lr_rate=1e-5)


# In[12]:


from imgaug import augmenters as iaa
import imgaug as ia


# In[14]:


def Augment_Pipeline():

    ia.seed(1)

    seq = iaa.Sequential([
        iaa.Sometimes(0.5,
            iaa.GaussianBlur(sigma=(0, 0.5))
        ),
        iaa.ContrastNormalization((0.75, 1.5)),
        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),
        iaa.PiecewiseAffine(scale=(0.01, 0.05))
    ], random_order=True) # apply augmenters in random order

    return seq


# In[18]:


seq = Augment_Pipeline()


# In[ ]:


seq.augment_batches


# In[105]:


TSNE(n_components=2).fit_transform()


# In[ ]:


samples = random.sample(range(1, len(test_paths)), params["batch_size"])
                test_batch_paths = [test_paths[i] for i in samples]
                test_batchx = np.array([load_data(path) for path in test_batch_paths])
                test_batchy = get_batch_label(test_batch_paths, params["path"])
                
              
                feed_dict = {Input: test_batchx, Label: test_batchy}

                test_accuracy, test_loss, test_summary_str = sess.run([accuracy, loss, summary_op], 
                                               feed_dict=feed_dict)
                test_summary_writer.add_summary(test_summary_str, idx)
                
                print('epoch : ' +str(epoch)+' step : '+str(idx)+' test_loss : '+str(test_loss)+
                      ' test_accuracy : '+str(test_accuracy))


# In[82]:


import matplotlib.font_manager as fm
import matplotlib.pyplot as plt

prop = fm.FontProperties(fname='kalpurush.ttf')
s = u"কৃষক জমিতে ধান চাষ করে"
x = 0.2
y = 0.2
plt.text(x, y, s, fontproperties=prop)
plt.show()



# In[99]:


import matplotlib.pyplot as PLT
from matplotlib.offsetbox import AnnotationBbox, OffsetImage
from matplotlib._png import read_png

fig = PLT.gcf()
fig.clf()
ax = PLT.subplot(111)

# add a first image
arr_hand = cv2.imread('/content/IndicCharacter_SSH/1/Bangla/001_01.bmp')
imagebox = OffsetImage(arr_hand, zoom=0.5)
xy = [.25, 0.45]               # coordinates to position this image

ab = AnnotationBbox(imagebox, xy
)                                  
ax.add_artist(ab)

# add second image
arr_vic =  cv2.imread('/content/IndicCharacter_SSH/1/Bangla/001_02.bmp')
imagebox = OffsetImage(arr_vic, zoom=0.2)
xy = [.6, .3]                  # coordinates to position 2nd image

ab = AnnotationBbox(imagebox, xy
)
ax.add_artist(ab)

# rest is just standard matplotlib boilerplate
ax.grid(True)
PLT.draw()
PLT.show()


